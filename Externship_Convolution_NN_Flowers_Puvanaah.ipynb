{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argumentation techniques is used to get a clearer picture, better quality, size\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argumentation techniques is applied to the train dataset\n",
    "#image \n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 images belonging to 10 classes.\n",
      "Found 30 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#applying the techniques to the dataset\n",
    "#size is changing the resolution in pixels\n",
    "#batch size, every epoch 32 images will be trained\n",
    "#class is categorical because there are different classes \n",
    "x_train = train_datagen.flow_from_directory(r\"C:\\Users\\PUVANAAH\\OneDrive\\Desktop\\trainset1\", target_size = (64, 64), batch_size = 16, class_mode = \"categorical\")\n",
    "x_test = train_datagen.flow_from_directory(r\"C:\\Users\\PUVANAAH\\OneDrive\\Desktop\\testset1\", target_size = (64, 64), batch_size = 16, class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64, 64 3(three channeled images)\n",
    "#arguments =(no of feature detectors,(size of feature detector,shape of image)\n",
    "cnn.add(Convolution2D(32,(3,3), input_shape = (64,64,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer is given by flatten\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding hidden layer\n",
    "cnn.add(Dense(units = 128, kernel_initializer = \"random_uniform\", activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer\n",
    "#units is 10 due to 10 classes\n",
    "cnn.add(Dense(units = 10 , kernel_initializer = \"random_uniform\", activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = \"rmsprop\",loss = \"categorical_crossentropy\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PUVANAAH\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 5s 1s/step - loss: 3.8809 - accuracy: 0.0857 - val_loss: 2.4571 - val_accuracy: 0.1667\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 2s 518ms/step - loss: 2.1493 - accuracy: 0.2714 - val_loss: 2.1603 - val_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 2s 636ms/step - loss: 1.8112 - accuracy: 0.3714 - val_loss: 2.0876 - val_accuracy: 0.2667\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 1.5823 - accuracy: 0.5000 - val_loss: 1.9058 - val_accuracy: 0.3333\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 1.4872 - accuracy: 0.4143 - val_loss: 1.9482 - val_accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 1.3031 - accuracy: 0.6000 - val_loss: 1.8720 - val_accuracy: 0.3000\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 2s 545ms/step - loss: 1.2772 - accuracy: 0.6429 - val_loss: 1.8759 - val_accuracy: 0.2667\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 1.0174 - accuracy: 0.7143 - val_loss: 2.3743 - val_accuracy: 0.2333\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 1.1173 - accuracy: 0.6286 - val_loss: 1.7266 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8726 - accuracy: 0.7571 - val_loss: 2.2082 - val_accuracy: 0.2667\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 4s 888ms/step - loss: 0.8165 - accuracy: 0.7000 - val_loss: 1.8904 - val_accuracy: 0.3333\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 2s 568ms/step - loss: 0.9498 - accuracy: 0.7286 - val_loss: 1.8765 - val_accuracy: 0.3667\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 2s 542ms/step - loss: 0.5772 - accuracy: 0.8429 - val_loss: 1.8831 - val_accuracy: 0.4667\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.5163 - accuracy: 0.8429 - val_loss: 1.9871 - val_accuracy: 0.3667\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.4736 - accuracy: 0.8429 - val_loss: 1.9466 - val_accuracy: 0.4333\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3738 - accuracy: 0.8857 - val_loss: 2.2442 - val_accuracy: 0.3667\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.3680 - accuracy: 0.8857 - val_loss: 2.7652 - val_accuracy: 0.2667\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 2s 634ms/step - loss: 0.3162 - accuracy: 0.9143 - val_loss: 2.0722 - val_accuracy: 0.3667\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 2s 561ms/step - loss: 0.5522 - accuracy: 0.8857 - val_loss: 2.0506 - val_accuracy: 0.4333\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.2192 - accuracy: 0.9571 - val_loss: 2.0495 - val_accuracy: 0.4333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28377592e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn.fit_generator(x_train, steps_per_epoch = no of images in trainset/batchsize, validation_data = x_test, validation_steps = no of images in testset/batchsize )\n",
    "cnn.fit_generator(x_train, steps_per_epoch = 70/16, epochs = 20, validation_data = x_test, validation_steps =  30/16 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Flower_10\\assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save(\"Flower_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Flower_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(r\"C:\\Users\\PUVANAAH\\OneDrive\\Desktop\\sun.jpg\",target_size = (64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.expand_dims(x,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daisy - 1000000000\n",
    "#dalia - 0100000000\n",
    "#gardenia - 0010000000\n",
    "#iris - 0001000000\n",
    "#lavender - 0000100000\n",
    "#lily - 000001000\n",
    "#orchid - 0000001000\n",
    "#rose - 1000000100\n",
    "#sunflower - 1000000010\n",
    "#tulip - 0000000001\n",
    "index = [\"daisy\", \"dalia\", \"gardenia\", \"iris\", \"lavender\", \"lily\", \"orchid\",\"rose\", \"sunflower\", \"tulip\"]\n",
    "prediction = index[np.argmax(pred[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunflower'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
